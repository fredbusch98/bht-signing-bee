# ‚úåÔ∏èSigning Bee üêù

## Getting Started

Folgende python packages m√ºssen installiert werden: `mediapipe`, `pillow`, `opencv-python`, `torch`
```bat
pip install mediapipe
pip install pillow
pip install opencv-python
pip install torch
```
`scikit-learn` (wird nur ben√∂tigt, wenn das Modell neu trainiert oder evaluiert werden soll)

```bat
pip install scikit-learn
```

## How to use
__Es wird eine Webcam ben√∂tigt!__ Nach dem Start der Anwendung wird ein Wort aus einer vordefinierten Liste von W√∂rtern angezeigt. Diese m√ºssen vor der Webcam in amerikanischer Zeichensprache (ASL) buchstabiert werden. Um Benutzern zu helfen, die nicht mit dem ASL-Alphabet vertraut sind, gibt es ein Hilfsbild in der Benutzeroberfl√§che, das die aktuell erforderliche Geb√§rde anzeigt. F√ºr den Fall, dass ein Buchstabe mal nicht erkannt wird und man zum n√§chsten √ºbergehen m√∂chte, haben wir auch Tastaturunterst√ºtzung implementiert, so dass Buchstaben √ºbersprungen werden k√∂nnen.

Navigiere zum Ordner `/src` und starte die Anwendung
```bat
cd src
python signing_bee.py
```

Application shortcuts
```
'z'   - Beendet die Anwendung
'j'   - Visualisierung der Hand Landmarks an- / ausschalten
'esc' - Vollbildmodus an- / ausschalten
```

## Expos√© - Signing Bee
Frederik Busch, Marc Waclaw, Lennart Reinke

Unsere Idee ist es, ein kleines Buchstabierspiel (engl. spelling bee) zu entwickeln. Buchstabieren jedoch nicht im herk√∂mmlichen Sinne mit der Stimme oder per Tastatureingabe, sondern mittels Sign Language.

Dem Nutzer wird ein zuf√§lliges Wort angezeigt und zus√§tzlich eine Hilfestellung in Form eines Bildes, das die zugeh√∂rige Geb√§rde f√ºr den erwarteten Buchstaben zeigt. Mit Hilfe eines Convolutional Neural Network (CNN) erkennen wir dann die Handgeste des Nutzers und zeigen den Buchstabier-Fortschritt in der Benutzeroberfl√§che an. Ziel ist es, eine Applikation zu entwickeln, die es erm√∂glicht Sign Language zu lernen und im Zuge des Projektes herauszufinden, welche Geb√§rden von unserem Algorithmus gut erkannt werden und welche weniger gut.

Zum Trainieren unseres CNNs verwenden wir den *Sign Language MNIST* Datensatz. Dieser enth√§lt etwa 35.000 28 x 28 Pixel Graustufenbilder von Geb√§rden des amerikanischen Sign Language Alphabets (27.455 Trainingsbilder, 7172 Testbilder). Der Datensatz schlie√üt die Geb√§rden f√ºr die Buchstaben *J* und *Z* aus, weil diese im Gegensatz zu allen anderen Buchstaben Bewegung erfordern. [1] Eine sinnvolle Erkennung dieser Geb√§rden ist mit dem Datensatz nicht m√∂glich. Deshalb werden auch wir in unserer Anwendung auf W√∂rter verzichten, die diese Buchstaben (*J*, *Z*) enthalten.

<img src="resources/images/asl-alphabet.png" alt="Konfusionsmatrix" width="400" height="400">

Wir werden das Projekt mit Hilfe von OpenCV, MediaPipe, PyTorch und tkinter umsetzen. OpenCV verwenden wir f√ºr die Bildverarbeitung, MediaPipe liefert n√ºtzliche Funktionen zur Hand-Detection, PyTorch nutzen wir zur Umsetzung unseres CNNs und tkinter f√ºr die Erstellung der Benutzeroberfl√§che. [2] [3] [4] [5]

Resources: 
* [1] Sign Language MNIST - https://www.kaggle.com/datasets/datamunge/sign-language-mnist
* [2] OpenCV - https://opencv.org/
* [3] MediaPipe - https://mediapipe.readthedocs.io/en/latest/solutions/hands.html
* [4] PyTorch - https://pytorch.org/docs/stable/index.html 
* [5] tkinter - https://docs.python.org/3/library/tkinter.html

## Neuer Datensatz
Das Projektteam hat festgestellt, dass die Ergebnisse mit dem Sign Language MNIST-Datensatz nicht den Anforderungen entsprochen haben. Die Hauptprobleme lagen in der hohen Fehleranf√§lligkeit bei der Erkennung und der Abh√§ngigkeit der Erkennungsqualit√§t von Lichtverh√§ltnissen und Hintergrundfarbe im Webcam-Frame. Diese Schwierigkeiten k√∂nnen auf die Verwendung von globalen Deskriptoren, in Form von 28x28 Graustufenbildern, zur√ºckgef√ºhrt werden. Diese Bilder bieten nur eine begrenzte Menge an Informationen f√ºr die Erkennung und unterscheiden sich m√∂glicherweise nicht ausreichend bei variierenden Bedingungen. Auch nach mehrfacher Erweiterung des Datensatzes um eigene Aufnahmen, so wie dem Versuch die Helligkeitswerte der bestehenden Daten zu augmentieren, war das Ergebnis nicht zufriedenstellend.

Aufgrund dieser Einschr√§nkungen haben wir uns entschieden, einen neuen Datensatz zu verwenden und das Feature von MediaPipe zur Erkennung von Hand-Landmarks einzuf√ºhren und auszunutzen. Bei diesem Ansatz verwenden wir 21 Hand-Landmarks pro Hand, wobei jeder Landmark durch normalisierte X-, Y- und Z-Koordinaten beschrieben wird. Dieser Wechsel erm√∂glicht es uns, von den globalen Deskriptoren der 28x28 Graustufenbilder auf detailliertere lokale Features umzusteigen. Ein weiterer Vorteil, der sich daraus ergeben hat ist eine deutliche Verringerung der Trainingsdauer, da wir nur noch 63 Werte pro Bild lernen, anstatt der vorherigen 784 Pixel pro Bild.

**Mathematische Erkl√§rung der lokalen Features:**

Landmarks beschreiben markante Punkte an der erkannten Hand in einem Bild, die als Referenzpunkte f√ºr die Analyse dienen. Hand-Landmarks sind spezifische Punkte, die typischerweise an den Fingerspitzen, Gelenken und dem Handgelenk liegen und verwendet werden, um die Position und Stellung der Hand zu analysieren, in unserem Fall zur Gestenerkennung. F√ºr jede Hand werden 21 Hand-Landmarks erkannt. Jeder dieser Landmarks besteht aus 3 Koordinaten (X, Y, Z). Daher k√∂nnen wir die Anzahl der lokalen Features wie folgt berechnen:

- Anzahl der Hand-Landmarks pro Hand: 21
- Anzahl der Koordinaten pro Landmark: 3

Die Gesamtanzahl der lokalen Features pro Hand ist daher:

$$
\text{Gesamtanzahl der lokalen Features} = \text{Anzahl der Hand-Landmarks} \times \text{Anzahl der Koordinaten pro Landmark}
$$

$$
\text{Gesamtanzahl der lokalen Features} = 21 \times 3 = 63
$$

Also besteht jede Handrepr√§sentation aus 63 lokalen Features (Koordinaten), die verwendet werden k√∂nnen, um die Handgesten zu erkennen. Dies bietet eine viel detailliertere und spezifischere Beschreibung der Handposition im Vergleich zu den urspr√ºnglichen globalen Deskriptoren.

<img src="resources/example-images/handlandmarks.jpg" alt="Hand-Landmarks-Darstellung" width="400" height="400">

F√ºr die Entwicklung und das Training unseres Modells haben wir zwei Datens√§tze verwendet:

- **Trainingsdatensatz**: [ASL (American Sign Language) Alphabet Dataset](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset?select=ASL_Alphabet_Dataset), der etwa 9000 Bilder pro Buchstabe enth√§lt.
- **Testdatensatz**: [ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet?select=asl_alphabet_train), der etwa 3000 Bilder pro Buchstabe umfasst.

Da beide Datens√§tze ausschlie√ülich Bilddaten enthalten, wir jedoch Hand-Landmarks zur Erkennung verwenden m√∂chten, mussten die Daten zun√§chst vorverarbeitet werden. Au√üerdem enth√§lt der Datensatz ausschlie√ülich rechth√§ndische Gesten. Hierzu haben wir das Skript `data_preprocessor.py` entwickelt. Dieses Skript liest alle Bilder aus den Datens√§tzen ein, erstellt zus√§tzlich ein horizontal gespiegeltes Duplikat (linkhsh√§dnische Geste) und nutzt dann MediaPipe zur Identifikation der Hand-Landmarks in beiden Bildern. Die erkannten Landmarks werden anschlie√üend in eine CSV-Datei geschrieben, wobei das zugeh√∂rige Label in der ersten Spalte als Pr√§fix angegeben wird. Weil nicht bei jedem Bild Hand-Landmarks erkannt werden konnten, reduzierte sich die Datenmenge der Originalbilder im Vorverarbeitungsprozess, da wir aber jedes Bild noch einmal horizontal gespiegelt haben ist die Gesamtmenge der Datenpunkte am Ende gr√∂√üer geworden als im urspr√ºnglichen Datensatz. Der finale Trainingsdatensatz enth√§lt nun insgesamt 281.896 Datenpunkte, im Durchschnitt 11.745 Datenpunkte pro Buchstabe. Der finale Testdatensatz enth√§lt insgesamt 114.401 Datenpunkte und weist im Durchschnitt 4.766 Datenpunkte pro Buchstabe auf. Diese Menge √ºbertrifft deutlich die Anzahl der Datenpunkte im Sign Language MNIST-Datensatz, der insgesamt nur 27.455 Datenpunkte und im Durchschnitt nur 1.145 Datenpunkte pro Buchstabe bereitstellte. Ein Nachteil dieses Vorverarbeitungsschrittes ist, das er ziemlich lange dauert. F√ºr den gesamten Bilddatensatz dauert es knapp drei Stunden auf einem Macbook Air 2020 M1. Daf√ºr ist das Training deutlich schneller unter anderem, aufgrund der verringerten Datenpunkte pro Bild (63 Hand-Landmark-Koordinaten vs. 784 Pixel).

Um sicherzustellen, dass die Trainingsdaten gleichm√§√üig verteilt sind, implementierten wir das Skript `label_counter.py`, das die Gesamtanzahl der Datenpunkte pro Label ermittelt. Es stellte sich heraus, dass die Buchstaben M und N nach der Vorverarbeitung signifikant weniger Datenpunkte aufwiesen als der Durchschnitt, mit lediglich 8.017 (M) bzw. 8.432 (N) Datenpunkten. Zur Balance des Trainingsdatensatzes wurden diese Buchstaben durch Duplizieren von Datenpunkten aufgestockt, um sicherzustellen, dass f√ºr jeden Buchstaben mindestens 11.000 Datenpunkte vorhanden sind.

### Architektur des HandGestureMLP-Modells
Wir haben uns, anders als im Kapitel `Expos√© - Signing Bee` beschrieben, gegen die Verwendung eines CNNs entschieden und stattdessen eine Multilayered Perceptron Architektur (MLP) umgesetzt. Ein MLP eignet sich gut f√ºr die Hand Landmark Daten, da diese tabellarisch vorliegen und keine r√§umlichen Beziehungen wie bei Bilddaten enthalten, die CNNs nutzen k√∂nnten. MLPs k√∂nnen effizienter mit solchen flachen Daten arbeiten, da sie auf die direkte Verarbeitung numerischer Merkmale ausgelegt sind. CNNs hingegen sind f√ºr Bildverarbeitung optimiert und ihre Faltungsoperationen sind f√ºr bereits extrahierte Landmark-Koordinaten unn√∂tig. Daher f√ºhrt die einfachere Struktur von MLPs zu schnellerem Training und besserer Anpassung bei diesem Anwendungsfall.

#### 1. **Input Layer:**
   - **`input_size=63`**: Das Input Layer erwartet einen Eingabefeature-Vektor mit 63 Merkmalen. Diese Merkmale stammen aus den vorverarbeiteten Hand-Landmark-Daten. Jeder Vektor stellt die Merkmale eines Handbildes oder einer Handgeste dar, wobei jedes Merkmal die Position eines bestimmten Punktes auf der Hand repr√§sentiert.

#### 2. **Hidden Layers:**
   - **`fc1`**: Das erste hidden Layer (`fc1`) ist ein vollverbundenes Layer (`nn.Linear(input_size, hidden_size)`). Es hat 128 Neuronen (`hidden_size`) und ist f√ºr die Umwandlung der Eingabefeatures in eine h√∂herdimensionale Repr√§sentation verantwortlich.
     - **Aktivierungsfunktion**: Nach der Linearen Transformation wird die ReLU-Aktivierungsfunktion (`F.relu`) angewendet, die nicht-lineare Beziehungen in den Daten modellieren kann.
     
   - **`fc2`**: Das zweite hidden Layer (`fc2`) ist ebenfalls ein vollverbundenes Layer, das die Ausgabe des ersten hidden Layer auf eine weitere h√∂here Dimension transformiert, wieder mit 128 Neuronen.
     - **Aktivierungsfunktion**: Auch hier wird die ReLU-Aktivierungsfunktion verwendet, um nicht-lineare Beziehungen zu modellieren.

#### 3. **Output Layer:**
   - **`fc3`**: Das Output Layer (`fc3`) ist ein vollverbundenes Layer, das die Ausgabe des letzten hidden Layer in eine Dimension von `num_classes` transformiert, was der Anzahl der verschiedenen Klassen entspricht, die im Handgestenerkennungssystem unterschieden werden sollen (in diesem Fall 24 Klassen = Buchstaben im Alphabet ohne J und Z).
     - **Aktivierungsfunktion**: Im Output Layer wird keine Aktivierungsfunktion angewendet. Stattdessen gibt das Layer Rohwerte oder Logits aus, die sp√§ter in der Verlustberechnung durch die Softmax-Funktion (implizit in der Crossentropy-Verlustfunktion) in Wahrscheinlichkeiten umgewandelt werden.

#### 4. **Zusammenfassung des Modells:**
   - **Modelltyp**: Multilayered Perceptron (MLP)
   - **Architektur**:
     - **Input**: 63 Features
     - **Hidden Layer 1**: 128 Neuronen, ReLU-Aktivierung
     - **Hidden Layer 2**: 128 Neuronen, ReLU-Aktivierung
     - **Output**: `num_classes` (24 Klassen)
   - **Gesamtzahl der Parameter**:
     - Die Anzahl der Parameter kann durch die Formel f√ºr fully-connected Layer berechnet werden: \( \text{Parameter} = (\text{Eingangsgr√∂√üe} \times \text{Ausgangsgr√∂√üe}) + \text{Ausgangsgr√∂√üe} \).
     - F√ºr `fc1`: $(63 * 128) + 128 = 8.128$
     - F√ºr `fc2`: $(128 * 128) + 128 = 16.512$
     - F√ºr `fc3`: $(128 * 24) + 24 = 3.104$
     - Insgesamt: $8128 + 16512 + 3104 = 27.844$ Parameter

Zum Vergleich dazu hatte unser vorheriges Modell, ein CNN, das mit dem Sign Language MNIST trainiert wurde insgesamt 423.961 Parameter. Das erkl√§rt auch nochmal, wieso das Training des neuen Modells weitaus schneller ist.

### Modell Training und Evaluation

- **Training**:
  - Der Trainingsprozess verwendet die Crossentropy-Verlustfunktion (`nn.CrossEntropyLoss`) und den Adam-Optimierer (`optim.Adam`). Der Optimierer aktualisiert die Modellparameter basierend auf dem Gradienten des Verlusts, der durch Backpropagation berechnet wird. Das Modell wurde √ºber 200 Epochen hinweg trainiert, um eine optimale Anpassung an die Daten zu erreichen. Nach wiederholtem Testen mit unterschiedlichen Anzahlen von Epochen erwies sich dieses Modell letztlich als das Beste.

- **Evaluation**:
  - Die Leistung des Modells wird w√§hrend des Trainings auf einem Validierungsdatensatz bewertet, um die Genauigkeit des Modells zu √ºberwachen und √úberanpassung (Overfitting) zu verhindern. Hierf√ºr wird w√§hrend dem Trainingsprozess der Trainingsdatensatz im Verh√§ltnis 80 (train) zu 20 (val) aufgeteilt. Au√üerdem gibt es noch ein zus√§tzliches Skript `metrics.py`, welches weitere Metriken zur Evaluierung unseres Modells berechnet auf Grundlage des im Kapitel zuvor beschriebenen Testdatensatzes: `Accuracy`, `Precision`, `Recall`, `F1-Score`, `Confusion Matrix` und die `ROC_AUC Curve`

### System Overview

<img src="resources/images/overview.png" alt="Konfusionsmatrix" width="2000" height="250">

## Ergebnisse

### Metriken

* Accuracy: **99,79%**  
* Precision: **99,78%**
* Recall: **99,79%**
* F1-Score: **99,79%**

Im folgenden haben wir zwei Diagramme, die die vorangestellten Werte kontextualisieren sollen. 

### Confusion Matrix

<img src="resources/results/confusion_matrix.jpg" alt="Konfusionsmatrix" width="600" height="600">
Wie anhand der Werte zu erkennen ist, ist die Rate der True Positives (TP) extrem hoch und die theoretische Leistung des Models sehr gut. Die Matrix zeigt allerdings ein paar interessante F√§lle auf, die in den Werten nicht zu erkennen sind. Beispielsweise die F√§lle in denen das K f√§lschlicherweise als V (13 F√§lle), das R als U (33 F√§lle), das P als Q (12 F√§lle) und das D als U (9 F√§lle) klassifiziert wurden, stechen hierbei heraus. W√§hrend die F√§lle K-V, R-U und D-U auf die hohe √Ñhnlichkeit der Gesten zur√ºckzuf√ºhren ist, ist der Fall P-Q nicht ganz so eindeutig, da sich diese beiden Gesten in Bildform doch deutlich zu unterscheiden scheinen.
<br><br>
<img src="resources/images/pq.png" alt="Konfusionsmatrix" width="200" height="200">

Das t√§uscht allerdings, denn obwohl sie aufgrund der Handrotation unterschiedlich aussehen, sind die Hand-Landmarks relativ zur Hand also unabh√§ngig von der Rotation der Hand im gesamten Bildkontext. Das bedeutet, die Gesten sind sich doch recht √§hnlich, denn lediglich der Mittelfinger ist beim Q angewinkelt und beim P eher ausgestreckt.

### ROC_AUC Curve

<img src="resources/results/roc_auc_curve.jpg" alt="ROC_AUC Kurve" width="600" height="600">
Trotz der hohen Werte f√ºr Accuracy, Precision, Recall und dem F1-Score ist es auff√§llig, dass der AUC-Wert f√ºr alle Werte gleich 1.00 ist. Dies w√ºrde aussagen, dass das Modell in 100% der F√§lle den richtigen Buchstaben erkennt, was mithilfe der Konfusionsmatrix widerlegt werden kann. Wir vermuten einen Rundungsfehler bei der Berechnung, konnten aber w√§hrend der Projektzeit keine L√∂sung oder eindeutige Erkl√§rung daf√ºr finden.

### Ergebnisse Sign Language MNIST
Zum Vergleich sind hier noch einmal einige Ergebnisse des ersten Modells, welches mit dem Sign Language MNIST trainiert wurde:
* Accuracy: **95,73%**
* Precision: **95,70%**
* Recall: **95,31%**
* F1-Score: **95,28%**

Diese Werte scheinen auch recht hoch, bei der tats√§chlichen Erkennung der Geb√§rden haben sie sich jedoch nicht bew√§hrt. Hier wurden viele Buchstaben, wie zuvor bereits erw√§hnt, nur schwer erkannt und bei geringsten Ver√§nderungen der Umgebungsverh√§ltnisse wurde die Erkennung teilweise nahezu unm√∂glich. Das die Werte dennoch so *gut* ausgefallen sind h√§ngt vermutlich damit zusammen, dass der Testdatensatz beim Sign Language MNIST bei sehr √§hnlichen Lichtverh√§ltnissen und Hintergrundbedingungen aufgenommen wurde, wie der Trainingsdatensatz, was dazuf√ºhrt, dass bei der Evaluierung die zuvor genannten Probleme nicht auffallen.

### Model Accuracy / Loss Over Epochs
Im Folgenden sind noch einmal zwei Diagramme zu sehen, die zum einen den Model-Loss und zum anderen die Model-Accuracy √ºber die Trainingsepochen darstellen.

<img src="resources/results/model_accuracy.svg" alt="Model Accuracy Over Epochs" width="600" height="600">
Bei der Accuracy ist ein schneller Anstieg zu Beginn des Trainings zu erkennen, der sich nach etwa 50 Epochen stabilisiert. Das Modell scheint also schnell zu lernen sowohl auf den Trainingsdaten, als auch den Validierungsdaten die korrekte Vorhersage zu treffen. Die leichten Schwankungen zeigen keine gro√üen Unterschiede zu den Trainingsdaten, was darauf hindeutet, dass wir kein starkes Overfitting betrieben haben.

<img src="resources/results/model_loss.svg" alt="Model Loss Over Epochs" width="600" height="600">
Ebenfalls ist beim Loss ein schneller Abfall des Werts zu erkennen, der sich nach etwa 50 Epochen bei einem konstanten Wert von ungef√§hr 0.01 stabilisert. Dies deutet auf ein gelunges Training hin, dass daf√ºr gesorgt hat, dass das Modell seine Fehlerrate einigerma√üen schnell minimieren und stabilisieren konnte.
<br><br>
Zusammengefasst l√§sst sich sagen, dass das Modell einen schnellen Lernprozess durchlaufen hat. Dieser schnelle Lernprozess, l√§sst uns vermuten, dass wir das Modell auch mit deutlichen weniger Epochen h√§tten trainieren k√∂nnen ohne signifikante Leistungseinbu√üen. Dar√ºber hinaus lassen sich bei der Modell-Accuracy keine auff√§lligen Anzeichen f√ºr Overfitting erkennen.

## Related Work

Unsere Anwendung zur Geb√§rdenspracherkennung baut auf bew√§hrten Methoden auf, die sich mit der Erkennung von Handgesten und Fingerspelling besch√§ftigen. Zwei zentrale Arbeiten haben uns dabei inspiriert und bieten einen guten Vergleich zu unserem Ansatz.

In der Studie von **[1]Byeongkeun Kang et al. (2015)** wird ein System vorgestellt, das mithilfe von Convolutional Neural Networks (CNNs) und Tiefenbildern das ASL-Fingerspelling-Alphabet in Echtzeit erkennt. Durch die Verwendung von Depth Maps k√∂nnen sie die Position und Tiefe der Hand pr√§zise erfassen, was das System robuster gegen√ºber variierenden Lichtverh√§ltnissen und Hautfarben macht. Das Resultat ist eine beeindruckende Erkennungsgenauigkeit von 99,99% bei bekannten Nutzern und bis zu 85,49% bei neuen Nutzern. Besonders interessant ist, dass sie durch das Hinzuf√ºgen von Daten verschiedener Nutzer die Leistung des Systems weiter steigern konnten, was die Bedeutung eines vielf√§ltigen Trainingsdatensatzes unterstreicht.

Das Paper von **[2]Fan Zhang et al. (2020)** beschriebt die Entwicklung von MediaPipe Hands, einem flexiblen Framework zur Echtzeit-Handverfolgung. MediaPipe nutzt eine zweistufige Erkennung: Zuerst wird die Handfl√§che durch einen schnellen Palm-Detektor erkannt, danach werden 21 Hand-Landmarks wie Gelenke und Fingerspitzen mithilfe eines trainierten neuronalen Netzwerks erfasst. Diese Methode funktioniert zuverl√§ssig auf einer Vielzahl von Ger√§ten, von Smartphones bis hin zu Webcams, und kommt ohne spezialisierte Hardware wie Tiefensensoren aus.

Zhang und sein Team zeigen in ihrem Paper, wie MediaPipe durch geschickte Modellarchitekturen und Optimierungen eine schnelle und pr√§zise Handverfolgung erm√∂glicht. Obwohl sie keine genauen Genauigkeitswerte f√ºr Fingerspelling angeben, unterstreicht das System seine Robustheit in verschiedenen Szenarien. MediaPipe ist damit eine vielseitige und effiziente L√∂sung, die sich besonders f√ºr Echtzeitanwendungen wie unsere eignet. F√ºr unsere L√∂sung haben wir die Hand-Landmarks, wie sie in diesem Paper beschrieben wurden verwendet.

Unser Ansatz, sowie der Ansatz von Byeongkeun Kang et. al. haben beide das gleiche Ziel ‚Äì eine zuverl√§ssige Geb√§rdenspracherkennung ‚Äì, verfolgen jedoch unterschiedliche Wege, um dies zu erreichen. W√§hrend Kang und sein Team auf spezialisierte Hardware setzen, liegt unser Fokus auf einer zug√§nglicheren und flexibleren L√∂sung, die auf einer breiteren Palette von Ger√§ten funktioniert. Beide Ans√§tze bieten wertvolle Einblicke in die Herausforderungen und M√∂glichkeiten der Geb√§rdenspracherkennung.

[1][Byeongkeun Kang et al. (2015)](https://arxiv.org/pdf/1509.03001) - Real-time Sign Language Fingerspelling Recognition using
Convolutional Neural Networks from Depth map (University of California, San Diego)

[2][Fan Zhang et al. (2020)](https://arxiv.org/pdf/2006.10214) - MediaPipe Hands: On-device Real-time Hand Tracking (Google Research Team)


## Download Links

* [Google Drive](https://drive.google.com/file/d/1Skzo5uSdSAMLlYa-qweol1fzWhmlK_hl/view?usp=drive_link) - Vorverarbeitete Hand Landmark Datenpunkte (Training / Test)

* [ASL (American Sign Language) Alphabet Dataset](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset?select=ASL_Alphabet_Dataset) - Trainingsbilder

* [ASL Alphabet](https://www.kaggle.com/datasets/grassknoted/asl-alphabet?select=asl_alphabet_train) - Testbilder
